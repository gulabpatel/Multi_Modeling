{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Multi_Modeling/blob/main/01_LLaVA_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/LLaVA\n",
        "%cd /content/LLaVA\n",
        "\n",
        "!pip install ninja\n",
        "!pip install flash-attn --no-build-isolation\n",
        "\n",
        "!pip install -e .\n",
        "\n",
        "# !python -m llava.serve.cli \\\n",
        "#     --model-path liuhaotian/llava-v1.5-7b \\\n",
        "#     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\n",
        "#     --load-4bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k4Onf2gP7UVx"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "threading.Thread(target=lambda: subprocess.run(['python3', '-m', 'llava.serve.controller', '--host', '0.0.0.0', '--port', '10000'], check=True), daemon=True).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XPB65dRW7UVx"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "command = [\n",
        "    'python3', '-m', 'llava.serve.model_worker',\n",
        "    '--host', '0.0.0.0',\n",
        "    '--controller', 'http://localhost:10000',\n",
        "    '--port', '40000',\n",
        "    '--worker', 'http://localhost:40000',\n",
        "    '--model-path', 'liuhaotian/llava-v1.5-7b'\n",
        "]\n",
        "threading.Thread(target=lambda: subprocess.run(command, check=True, shell=False), daemon=True).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n9JeAvy87UVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9c685a-9424-400e-89e2-cd255efe4b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-10-09 10:15:51,894] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-10-09 10:15:56.443778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-10-09 10:16:01 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=True, moderate=False, embed=False)\n",
            "2023-10-09 10:16:01 | INFO | gradio_web_server | Models: []\n",
            "2023-10-09 10:16:01 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=True, moderate=False, embed=False)\n",
            "2023-10-09 10:16:02 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\n",
            "2023-10-09 10:16:10 | INFO | stdout | Running on public URL: https://ac27eb8c435f2c0a5b.gradio.live\n",
            "2023-10-09 10:16:10 | INFO | stdout | \n",
            "2023-10-09 10:16:10 | INFO | stdout | This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2023-10-09 10:16:37 | INFO | gradio_web_server | load_demo. ip: 172.31.30.194\n",
            "2023-10-09 10:16:37 | INFO | gradio_web_server | Models: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-11 (<lambda>):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-3-a4a98ac65db6>\", line 11, in <lambda>\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['python3', '-m', 'llava.serve.model_worker', '--host', '0.0.0.0', '--controller', 'http://localhost:10000', '--port', '40000', '--worker', 'http://localhost:40000', '--model-path', 'liuhaotian/llava-v1.5-7b']' died with <Signals.SIGKILL: 9>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-09 10:19:12 | INFO | gradio_web_server | add_text. ip: 172.31.30.194. len: 32\n",
            "2023-10-09 10:19:15 | INFO | gradio_web_server | http_bot. ip: 172.31.30.194\n",
            "2023-10-09 10:19:15 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:20:24 | INFO | gradio_web_server | add_text. ip: 172.31.30.194. len: 38\n",
            "2023-10-09 10:20:27 | INFO | gradio_web_server | http_bot. ip: 172.31.30.194\n",
            "2023-10-09 10:20:27 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:20:36 | INFO | gradio_web_server | regenerate. ip: 172.31.30.194\n",
            "2023-10-09 10:20:39 | INFO | gradio_web_server | http_bot. ip: 172.31.30.194\n",
            "2023-10-09 10:20:39 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:20:45 | INFO | gradio_web_server | regenerate. ip: 172.31.30.194\n",
            "2023-10-09 10:20:48 | INFO | gradio_web_server | http_bot. ip: 172.31.30.194\n",
            "2023-10-09 10:20:48 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:20:59 | INFO | gradio_web_server | clear_history. ip: 172.31.30.194\n",
            "2023-10-09 10:21:15 | INFO | gradio_web_server | add_text. ip: 172.31.30.194. len: 22\n",
            "2023-10-09 10:21:18 | INFO | gradio_web_server | http_bot. ip: 172.31.30.194\n",
            "2023-10-09 10:21:18 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:29:34 | INFO | gradio_web_server | load_demo. ip: 172.31.51.42\n",
            "2023-10-09 10:29:34 | INFO | gradio_web_server | Models: []\n",
            "2023-10-09 10:29:58 | INFO | gradio_web_server | add_text. ip: 172.31.51.42. len: 30\n",
            "2023-10-09 10:30:01 | INFO | gradio_web_server | http_bot. ip: 172.31.51.42\n",
            "2023-10-09 10:30:01 | INFO | gradio_web_server | model_name: , worker_addr: \n",
            "2023-10-09 10:30:08 | INFO | stdout | Keyboard interruption in main thread... closing server.\n",
            "2023-10-09 10:30:08 | ERROR | stderr | Traceback (most recent call last):\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2058, in block_thread\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     time.sleep(0.1)\n",
            "2023-10-09 10:30:08 | ERROR | stderr | KeyboardInterrupt\n",
            "2023-10-09 10:30:08 | ERROR | stderr | \n",
            "2023-10-09 10:30:08 | ERROR | stderr | During handling of the above exception, another exception occurred:\n",
            "2023-10-09 10:30:08 | ERROR | stderr | \n",
            "2023-10-09 10:30:08 | ERROR | stderr | Traceback (most recent call last):\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     return _run_code(code, main_globals, None,\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     exec(code, run_globals)\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/content/LLaVA/llava/serve/gradio_web_server.py\", line 413, in <module>\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     demo.queue(concurrency_count=args.concurrency_count, status_update_rate=10,\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1975, in launch\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     self.block_thread()\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2061, in block_thread\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     self.server.close()\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 43, in close\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     self.thread.join()\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     self._wait_for_tstate_lock()\n",
            "2023-10-09 10:30:08 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "2023-10-09 10:30:08 | ERROR | stderr |     if lock.acquire(block, timeout):\n",
            "2023-10-09 10:30:08 | ERROR | stderr | KeyboardInterrupt\n",
            "2023-10-09 10:30:08 | INFO | stdout | Killing tunnel 0.0.0.0:7860 <> https://ac27eb8c435f2c0a5b.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --share"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ep8cIn627fqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}