{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMisrPx4Qpogd1fFcGx7kiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Multi_Modeling/blob/main/02_cogvlm_chat_hf_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CogVLM is a powerful open-source visual language model (VLM). CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. CogVLM can also chat with you about images."
      ],
      "metadata": {
        "id": "VcYgvVNE_g4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pke1eOw-DiX",
        "outputId": "7ba4f792-e366-46dc-a966-9172dadfc1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Collecting transformers==4.35.0\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.24.1\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.99\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.7.0\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.22.post7\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (5.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, einops, huggingface-hub, xformers, tokenizers, accelerate, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.24.1 einops-0.7.0 huggingface-hub-0.17.3 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0 xformers-0.0.22.post7\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.1.0 transformers==4.35.0 accelerate==0.24.1 sentencepiece==0.1.99 einops==0.7.0 xformers==0.0.22.post7 triton==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'THUDM/cogvlm-chat-hf',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").to('cuda').eval()"
      ],
      "metadata": {
        "id": "O1Rqa4lE-RI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat example\n",
        "query = 'Describe this image'\n",
        "image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')\n",
        "inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode\n",
        "inputs = {\n",
        "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
        "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
        "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
        "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
        "}\n"
      ],
      "metadata": {
        "id": "_4w8LoXi-Wkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "    print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "# This image captures a moment from a basketball game. Two players are prominently featured: one wearing a yellow jersey with the number\n",
        "# 24 and the word 'Lakers' written on it, and the other wearing a navy blue jersey with the word 'Washington' and the number 34. The player\n",
        "# in yellow is holding a basketball and appears to be dribbling it, while the player in navy blue is reaching out with his arm, possibly\n",
        "# trying to block or defend. The background shows a filled stadium with spectators, indicating that this is a professional game.</s>\n"
      ],
      "metadata": {
        "id": "z6F9X4-g-Fki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vqa example\n",
        "query = 'How many houses are there in this cartoon?'\n",
        "image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/3.jpg?raw=true', stream=True).raw).convert('RGB')\n",
        "inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image], template_version='vqa')   # vqa mode\n",
        "inputs = {\n",
        "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
        "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
        "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
        "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
        "}\n",
        "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "    print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "NsvpBbdo-jmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dispatch the model into multiple GPUs with smaller VRAM. This is an example for you have two 24GB GPU and 16GB CPU memory. you can change the arguments of infer_auto_device_map with your own setting."
      ],
      "metadata": {
        "id": "E35ap7Cc_Wr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
        "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        'THUDM/cogvlm-chat-hf',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'}, no_split_module_classes='CogVLMDecoderLayer')\n",
        "model = load_checkpoint_and_dispatch(\n",
        "    model,\n",
        "    'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = model.eval()\n",
        "\n",
        "# check device for weights if u want to\n",
        "for n, p in model.named_parameters():\n",
        "    print(f\"{n}: {p.device}\")\n",
        "\n",
        "# chat example\n",
        "query = 'Describe this image'\n",
        "image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')\n",
        "inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode\n",
        "inputs = {\n",
        "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
        "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
        "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
        "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
        "}\n",
        "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "    print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "JDuxo72z_VOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}